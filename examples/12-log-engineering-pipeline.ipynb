{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Data Engineering Pipeline ‚Äî Log Processing with loclean\n",
    "\n",
    "A production-oriented **data engineering** notebook showing how `loclean` handles\n",
    "unstructured log data: parsing, shredding into relational tables, structured\n",
    "extraction, and quality validation.\n",
    "\n",
    "**Pipeline:** Raw logs ‚Üí Structured Extraction ‚Üí Log Shredding ‚Üí Quality Gates ‚Üí Clean relational tables\n",
    "\n",
    "> **Model:** `qwen2.5-coder:1.5b` (lightweight, code-specialised).\n",
    "> Swap to `qwen2.5-coder:7b` for complex log formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "import loclean\n",
    "\n",
    "MODEL = \"qwen2.5-coder:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ¬∑ Raw Log Data\n",
    "\n",
    "Simulates a mixed-format log ingestion: web server access logs with embedded\n",
    "user agents, IPs, status codes, and response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pl.DataFrame(\n",
    "    {\n",
    "        \"log_entry\": [\n",
    "            (\n",
    "                \"192.168.1.10 - admin \"\n",
    "                \"[2024-01-15 08:23:45] \"\n",
    "                '\"GET /api/users HTTP/1.1\" '\n",
    "                '200 1523 0.045s \"Mozilla/5.0\"'\n",
    "            ),\n",
    "            (\n",
    "                \"10.0.0.55 - jsmith \"\n",
    "                \"[2024-01-15 08:24:01] \"\n",
    "                '\"POST /api/orders HTTP/1.1\" '\n",
    "                '201 892 0.123s \"curl/7.68\"'\n",
    "            ),\n",
    "            (\n",
    "                \"172.16.0.99 - - \"\n",
    "                \"[2024-01-15 08:24:15] \"\n",
    "                '\"GET /health HTTP/1.1\" '\n",
    "                '200 23 0.002s \"kube-probe/1.28\"'\n",
    "            ),\n",
    "            (\n",
    "                \"192.168.1.10 - admin \"\n",
    "                \"[2024-01-15 08:25:30] \"\n",
    "                '\"DELETE /api/users/42 HTTP/1.1\" '\n",
    "                '403 156 0.015s \"Mozilla/5.0\"'\n",
    "            ),\n",
    "            (\n",
    "                \"10.0.0.55 - jsmith \"\n",
    "                \"[2024-01-15 08:26:00] \"\n",
    "                '\"PUT /api/orders/100 HTTP/1.1\" '\n",
    "                '200 445 0.089s \"curl/7.68\"'\n",
    "            ),\n",
    "            (\n",
    "                \"192.168.2.1 - ops \"\n",
    "                \"[2024-01-15 08:27:10] \"\n",
    "                '\"GET /metrics HTTP/1.1\" '\n",
    "                '200 8921 0.234s \"Prometheus/2.45\"'\n",
    "            ),\n",
    "            (\n",
    "                \"10.0.0.55 - jsmith \"\n",
    "                \"[2024-01-15 08:28:00] \"\n",
    "                '\"POST /api/orders HTTP/1.1\" '\n",
    "                '500 234 2.105s \"curl/7.68\"'\n",
    "            ),\n",
    "            (\n",
    "                \"172.16.0.99 - - \"\n",
    "                \"[2024-01-15 08:29:00] \"\n",
    "                '\"GET /health HTTP/1.1\" '\n",
    "                '200 23 0.001s \"kube-probe/1.28\"'\n",
    "            ),\n",
    "            (\n",
    "                \"192.168.1.10 - admin \"\n",
    "                \"[2024-01-15 08:30:15] \"\n",
    "                '\"GET /api/users?page=2 HTTP/1.1\" '\n",
    "                '200 3201 0.067s \"Mozilla/5.0\"'\n",
    "            ),\n",
    "            (\n",
    "                \"10.0.0.88 - deploy \"\n",
    "                \"[2024-01-15 08:31:00] \"\n",
    "                '\"POST /api/deploy HTTP/1.1\" '\n",
    "                '202 567 1.456s \"Jenkins/2.401\"'\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Sample log entries: {logs.shape}\")\n",
    "logs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ¬∑ Structured Extraction ‚Äî parse fields from logs\n",
    "\n",
    "Use `loclean.extract()` with a Pydantic schema to parse structured fields\n",
    "from each log line. The LLM handles format variations automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AccessLog(BaseModel):\n",
    "    ip: str\n",
    "    user: str\n",
    "    timestamp: str\n",
    "    method: str\n",
    "    path: str\n",
    "    status_code: int\n",
    "    response_bytes: int\n",
    "    response_time_s: float\n",
    "    user_agent: str\n",
    "\n",
    "\n",
    "parsed = loclean.extract(\n",
    "    logs,\n",
    "    AccessLog,\n",
    "    target_col=\"log_entry\",\n",
    "    output_type=\"dataframe\",\n",
    "    model=MODEL,\n",
    ")\n",
    "\n",
    "print(f\"Parsed {parsed.shape[0]} entries into {parsed.shape[1]} columns\")\n",
    "parsed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ¬∑ Compiled Extraction ‚Äî high-performance parsing\n",
    "\n",
    "`extract_compiled()` generates a native Python function (no LLM at runtime),\n",
    "for 100x faster parsing on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_result = loclean.extract_compiled(\n",
    "    logs,\n",
    "    \"log_entry\",\n",
    "    AccessLog,\n",
    "    instruction=\"Parse the access log entry into structured fields.\",\n",
    "    max_retries=5,\n",
    "    model=MODEL,\n",
    ")\n",
    "\n",
    "print(f\"Compiled extraction: {compiled_result.shape}\")\n",
    "compiled_result.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ¬∑ Log Shredding ‚Äî relational table decomposition\n",
    "\n",
    "The LLM infers a relational schema and generates a parser to split\n",
    "log entries into normalised tables (e.g. `requests`, `users`). This is\n",
    "useful for loading into a data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = loclean.shred_to_relations(\n",
    "    logs,\n",
    "    \"log_entry\",\n",
    "    sample_size=10,\n",
    "    max_retries=5,\n",
    "    model=MODEL,\n",
    ")\n",
    "\n",
    "print(f\"Shredded into {len(tables)} tables:\\n\")\n",
    "for name, df in tables.items():\n",
    "    print(f\"  üìã {name}: {df.shape}\")\n",
    "    print(f\"     Columns: {list(df.columns)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect each shredded table\n",
    "for name, df in tables.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Table: {name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 ¬∑ Entity Resolution ‚Äî canonicalize user identifiers\n",
    "\n",
    "User fields like `\"admin\"`, `\"jsmith\"`, `\"-\"` (anonymous) need normalisation.\n",
    "This helps build consistent user activity tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pl.DataFrame(\n",
    "    {\n",
    "        \"user_raw\": [\n",
    "            \"admin\",\n",
    "            \"Admin\",\n",
    "            \"ADMIN\",\n",
    "            \"administrator\",\n",
    "            \"jsmith\",\n",
    "            \"j.smith\",\n",
    "            \"john.smith\",\n",
    "            \"ops\",\n",
    "            \"operations\",\n",
    "            \"deploy\",\n",
    "            \"deployer\",\n",
    "            \"-\",\n",
    "            \"anonymous\",\n",
    "            \"(none)\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "resolved = loclean.resolve_entities(\n",
    "    user_df,\n",
    "    \"user_raw\",\n",
    "    threshold=0.7,\n",
    "    model=MODEL,\n",
    ")\n",
    "\n",
    "print(\"User entity resolution:\")\n",
    "resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 ¬∑ Quality Gates ‚Äî validate processed data\n",
    "\n",
    "Before loading into the warehouse, validate the data against\n",
    "business rules defined in plain English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = loclean.validate_quality(\n",
    "    logs,\n",
    "    rules=[\n",
    "        \"Each log entry must contain an IP address\",\n",
    "        (\"HTTP status codes must be 3-digit numbers between 100 and 599\"),\n",
    "        (\"Timestamps must follow ISO-8601 or common datetime format\"),\n",
    "    ],\n",
    "    sample_size=10,\n",
    "    model=MODEL,\n",
    ")\n",
    "\n",
    "rate = quality[\"compliance_rate\"]\n",
    "status = \"‚úÖ PASS\" if rate >= 0.95 else \"‚ùå FAIL\"\n",
    "print(f\"Quality gate result: {status}\")\n",
    "print(f\"Compliance: {rate:.0%}\")\n",
    "\n",
    "if quality[\"failures\"]:\n",
    "    print(f\"\\nTop failures ({len(quality['failures'])}):\\n\")\n",
    "    for f in quality[\"failures\"][:5]:\n",
    "        idx = f.get(\"row_index\", \"?\")\n",
    "        rule = f.get(\"rule\", \"\")\n",
    "        reason = f.get(\"reason\", \"\")\n",
    "        print(f\"  Row {idx}: {rule} ‚Üí {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 ¬∑ PII Scrubbing ‚Äî before data lake ingestion\n",
    "\n",
    "Scrub IP addresses and usernames before storing in the data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_logs = logs.select(\"log_entry\").to_series().to_list()\n",
    "\n",
    "scrubbed = loclean.scrub(\n",
    "    logs,\n",
    "    target_col=\"log_entry\",\n",
    "    mode=\"mask\",\n",
    "    model=MODEL,\n",
    ")\n",
    "\n",
    "scrubbed_logs = scrubbed.select(\"log_entry\").to_series().to_list()\n",
    "\n",
    "print(\"Scrubbed log entries (PII masked):\")\n",
    "for orig, masked in zip(original_logs[:3], scrubbed_logs[:3], strict=True):\n",
    "    print(f\"  Before: {orig}\")\n",
    "    print(f\"  After:  {masked}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Step | API | Use Case |\n",
    "|------|-----|----------|\n",
    "| Structured Extraction | `loclean.extract()` | Parse log fields into columns |\n",
    "| Compiled Extraction | `loclean.extract_compiled()` | High-perf native parsing |\n",
    "| Log Shredding | `loclean.shred_to_relations()` | Split into normalised tables |\n",
    "| Entity Resolution | `loclean.resolve_entities()` | Canonicalize user IDs |\n",
    "| Quality Gates | `loclean.validate_quality()` | Pre-load data validation |\n",
    "| PII Scrubbing | `loclean.scrub()` | Mask PII before lake ingestion |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
